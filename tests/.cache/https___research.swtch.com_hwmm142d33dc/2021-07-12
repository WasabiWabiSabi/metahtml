<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8">
    <title>research!rsc: Hardware Memory Models (Memory Models, Part 1)</title>
    <link rel="alternate" type="application/atom+xml" title="research!rsc - Atom" href="http://research.swtch.com/feed.atom" />
    
<meta name="viewport" content="width=device-width, initial-scale=1">
<style>
  body {
    padding: 0;
    margin: 0;
    font-size: 100%;
    font-family: serif;
  }
  @media print {
    img {page-break-inside: avoid;}
    div.nosplit {page-break-inside: avoid;}
  }
  img.center {
    display: block;
    margin: 0 auto;
  }
  img.resizable {
    max-width: 100%;
    height: auto;
  }
  .pad {
    padding-top: 1em;
    padding-bottom: 1em;
  }
  a.anchor, a.back, a.footnote {
    color: black !important;
    text-decoration: none !important;
  }
  a.back {
    font-size: 50%;
  }
  @media print {
    a.back {display: none;}
  }
  .header {
    height: 1.25em;
    background-color: #dff;
    margin: 0;
    padding: 0.1em 0.1em 0.2em;
    border-top: 1px solid black;
    border-bottom: 1px solid #8ff;
  }
  .header h3 {
    margin: 0;
    padding: 0 2em;
    display: inline-block;
    padding-right: 2em;
    font-style: italic;
    font-size: 90%;
  }
  .rss {
    float: right;
    padding-top: 0.2em;
    padding-right: 2em;
    display: none;
  }
  .toc {
    margin-top: 2em;
  }
  .toc-title {
    font-family: serif;
    font-size: 300%;
    line-height: 50%;
  }
  .toc-subtitle {
    display: block;
    margin-bottom: 1em;
    font-size: 83%;
  }
  @media only screen and (max-width: 550px) { .toc-subtitle { display: none; } }
  .header h3 a {
    color: black;
  }
  .header h4 {
    margin: 0;
    padding: 0;
    display: inline-block;
    font-weight: normal;
    font-size: 83%;
  }
  @media only screen and (max-width: 550px) { .header h4 { display: none; } }
  .main {
    padding: 0 2em;
  }
  @media only screen and (max-width: 479px) { .article { font-size: 120%; } }
  .article h1 {
    text-align: center;
    font-size: 200%;
  }
  .copyright {
    font-size: 83%;
  }
  .subtitle {
      font-size: 65%;
  }
  .normal {
    font-size: medium;
    font-weight: normal;
  }
  .when {
    text-align: center;
    font-size: 100%;
    margin: 0;
    padding: 0;
  }
  .when p {
    margin: 0;
    padding: 0;
  }
  .article h2 {
    font-size: 125%;
    padding-top: 0.25em;
  }
  .article h3 {
    font-size: 100%;
  }
  pre {
    margin-left: 4em;
    margin-right: 4em;
  }
  pre, code {
    font-family: 'Inconsolata', monospace;
    font-size: 100%;
  }
  .footer {
    margin-top: 10px;
    font-size: 83%;
    font-family: sans-serif;
  }
  .comments {
    margin-top: 2em;
    background-color: #ffe;
    border-top: 1px solid #aa4;
    border-left: 1px solid #aa4;
    border-right: 1px solid #aa4;
  }
  .comments-header {
    padding: 0 5px 0 5px;
  }
  .comments-header p {
    padding: 0;
    margin: 3px 0 0 0;
  }
  .comments-body {
    padding: 5px 5px 5px 5px;
  }
  #plus-comments {
    border-bottom: 1px dotted #ccc;
  }
  .plus-comment {
    width: 100%;
    font-size: 14px;
    border-top: 1px dotted #ccc;
  }
  .me {
    background-color: #eec;
  }
  .plus-comment ul {
    margin: 0;
    padding: 0;
    list-style: none;
    width: 100%;
    display: inline-block;
  }
  .comment-when {
    color:#999;
    width:auto;
    padding:0 5px;
  }
  .old {
    font-size: 83%;
  }
  .plus-comment ul li {
    display: inline-block;
    vertical-align: top;
    margin-top: 5px;
    margin-bottom: 5px;
    padding: 0;
  }
  .plus-icon {
    width: 45px;
  }
  .plus-img {
    float: left;
    margin: 4px 4px 4px 4px;
    width: 32px;
    height: 32px;
  }
  .plus-comment p {
    margin: 0;
    padding: 0;
  }
  .plus-clear {
    clear: left;
  }
  .toc-when {
    font-size: 83%;
    color: #999;
  }
  .toc {
    list-style: none;
  }
  .toc li {
    margin-bottom: 0.5em;
  }
  .toc-head {
    margin-bottom: 1em !important;
    font-size: 117%;
  }
  .toc-summary {
    margin-left: 2em;
  }
  .favorite {
    font-weight: bold;
  }
  .article p, .article ol {
    line-height: 144%;
  }
  sup, sub {
    vertical-align: baseline;
    position: relative;
    font-size: 83%;
  }
  sup {
    bottom: 1ex;
  }
  sub {
    top: 0.8ex;
  }

  .main {
    position: relative;
    margin: 0 auto;
    padding: 0;
    width: 900px;
  }
  @media only screen and (min-width: 768px) and (max-width: 959px) { .main { width: 708px; } }
  @media only screen and (min-width: 640px) and (max-width: 767px) { .main { width: 580px; } }
  @media only screen and (min-width: 480px) and (max-width: 639px) { .main { width: 420px; } }
  @media only screen and (max-width: 479px) { .main { width: 300px; } }

</style>

  </head>
  <body>
    
<div class="header">
  <h3><a href="/">research!rsc</a></h3>
  <h4>Thoughts and links about programming,
    by <a href="https://swtch.com/~rsc/" rel="author">Russ Cox</a> </h4>
  <a class="rss" href="/feed.atom"><img src="/feed-icon-14x14.png" /></a>
</div>

    <div class="main">
      <div class="article">
        <h1>Hardware Memory Models
        
        <div class="subtitle">(<i><a href="mm">Memory Models</a>, Part 1</i>)</div>
        
        <div class="normal">
        <div class="when">
          
            Posted on Tuesday, June 29, 2021.
            
           <font size="-1"><a href="hwmm.pdf">PDF</a></font>
        </div>
        </div>
        </h1>
        <a class=anchor href="#introduction"><h2 id="introduction">Introduction: A Fairy Tale, Ending</h2></a>


<p>
A long time ago, when everyone wrote single-threaded programs,
one of the most effective ways to make a program run faster was to sit back and do nothing.
Optimizations in the next generation of hardware and the next generation of compilers
would make the program run exactly as before, just faster.
During this fairy-tale period,
there was an easy test for whether an optimization was valid:
if programmers couldn't tell the difference (except for the speedup)
between the unoptimized and optimized execution of a valid program,
then the optimization was valid.
That is, <i>valid optimizations do not change the behavior of valid programs.</i>

<p>
One sad day, years ago, the hardware engineers' magic spells
for making individual processors faster and faster stopped working.
In response, they found a new magic spell that let them
create computers with more and more processors,
and operating systems exposed this hardware parallelism
to programmers in the abstraction of threads.
This new magic spell—multiple processors made available in the form of
operating-system threads—worked much better for the hardware engineers,
but it created significant problems for language designers, compiler writers and programmers.

<p>
Many hardware and compiler optimizations that were invisible
(and therefore valid) in single-threaded programs
produce visible changes in multithreaded programs.
If valid optimizations do not change the behavior of valid programs,
then either these optimizations or the existing programs must be declared invalid.
Which will it be, and how can we decide?

<p>
Here is a simple example program in a C-like language.
In this program and in all programs we will consider, all variables are initially set to zero.
<pre>// Thread 1           // Thread 2
x = 1;                while(done == 0) { /* loop */ }
done = 1;             print(x);
</pre>


<p>
If thread 1 and thread 2, each running on its own dedicated processor,
both run to completion, can this program print 0?

<p>
It depends.
It depends on the hardware, and it depends on the compiler.
A direct line-for-line translation to assembly
run on an x86 multiprocessor will always print 1.
But a direct line-for-line translation to assembly run on an ARM or POWER
multiprocessor can print 0.
Also, no matter what the underlying hardware,
standard compiler optimizations could make this program
print 0 or go into an infinite loop.

<p>
“It depends” is not a happy ending.
Programmers need a clear answer to whether a program will
continue to work with new hardware and new compilers.
And hardware designers and compiler developers
need a clear answer to how precisely the hardware and compiled code
are allowed to behave when executing a given program.
Because the main issue here is the visibility and consistency
of changes to data stored in memory,
that contract is called the memory consistency model
or just <i>memory model</i>.

<p>
Originally, the goal of a memory model was to define
what hardware guaranteed to a programmer writing assembly code.
In that setting, the compiler is not involved.
Twenty-five years ago, people started trying to write memory models
defining what a high-level programming language like Java or C++
guarantees to programmers writing code in that language.
Including the compiler in the model makes the job of defining
a reasonable model much more complicated.

<p>
This is the <a href="mm">first of a pair of posts</a> about hardware memory models
and programming language memory models, respectively.
My goal in writing these posts is to build up background for
discussing potential <a href="gomm">changes we might want to make in Go's memory model</a>.
But to understand where Go is and where we might want to head,
first we have to understand where other hardware memory models
and language memory models are today and the precarious paths
they took to get there.

<p>
Again, this post is about hardware.
Let's assume we are writing assembly language for a multiprocessor computer.
What guarantees do programmers need from the computer hardware
in order to write correct programs?
Computer scientists have been searching for good answers to this question
for over forty years.
<a class=anchor href="#sc"><h2 id="sc">Sequential Consistency</h2></a>


<p>
Leslie Lamport's 1979 paper “<a href="https://www.microsoft.com/en-us/research/publication/make-multiprocessor-computer-correctly-executes-multiprocess-programs/">How to Make a Multiprocessor Computer
That Correctly Executes Multiprocess Programs</a>” introduced the
concept of sequential consistency:<blockquote>

<p>
The customary approach to designing and proving the correctness
of multiprocess algorithms for such a computer assumes that
the following condition is satisfied: the result of any execution
is the same as if the operations of all the processors were executed
in some sequential order, and the operations of each individual
processor appear in this sequence in the order specified by its program.
A multiprocessor satisfying this condition will be called <i>sequentially consistent</i>.</blockquote>

<p>
Today we talk about not just computer hardware but also programming languages
guaranteeing sequential consistency, when the only possible executions
of a program correspond to some kind of interleaving of thread
operations into a sequential execution.
Sequential consistency is usually considered the ideal model,
the one most natural for programmers to work with.
It lets you assume programs execute in the order they appear on the page,
and the executions of individual threads are simply interleaved
in some order but not otherwise rearranged.

<p>
One might reasonably question whether sequential consistency
<i>should</i> be the ideal model, but that's beyond the scope of this post.
I will note only that considering all possible thread
interleavings remains, today as in 1979,
“the customary approach to designing and proving the correctness of multiprocess algorithms.”
In the intervening four decades, nothing has replaced it.

<p>
Earlier I asked whether this program can print 0:
<pre>// Thread 1           // Thread 2
x = 1;                while(done == 0) { /* loop */ }
done = 1;             print(x);
</pre>


<p>
To make the program a bit easier to analyze, let's remove the loop and the print
and ask about the possible results from reading the shared variables:<blockquote>

<p>
<i>Litmus Test: Message Passing</i><br>
Can this program see <code>r1</code> <code>=</code> <code>1</code>, <code>r2</code> <code>=</code> <code>0</code>?</blockquote>
<pre>// Thread 1           // Thread 2
x = 1                 r1 = y
y = 1                 r2 = x
</pre>


<p>
We assume every example starts with all shared variables set to zero.
Because we're trying to establish what hardware is allowed to do,
we assume that each thread is executing on its own dedicated processor
and that there's no compiler to reorder what happens in the thread:
the instructions in the listings are the instructions the processor executes.
The name <code>r</code><i>N</i> denotes a thread-local register, not a shared variable,
and we ask whether a particular setting of thread-local registers is possible
at the end of an execution.

<p>
This kind of question about execution results
for a sample program is called a <i>litmus test</i>.
Because it has a binary answer—is this outcome possible or not?—a litmus test gives us a
clear way to distinguish memory models:
if one model allows a particular execution and another does not,
the two models are clearly different.
Unfortunately, as we will see later,
the answer a particular model gives to a particular litmus test
is often surprising.

<p>
If the execution of this litmus test is sequentially consistent, there are only six possible interleavings:

<p>
<img name="mem-litmus" class="center pad" width=492 height=169 src="mem-litmus.png" srcset="mem-litmus.png 1x, mem-litmus@2x.png 2x, mem-litmus@3x.png 3x, mem-litmus@4x.png 4x">

<p>
Since no interleaving ends with <code>r1</code> <code>=</code> <code>1</code>, <code>r2</code> <code>=</code> <code>0</code>, that result is disallowed.
That is, on sequentially consistent hardware, the answer to the litmus test—can this program see <code>r1</code> <code>=</code> <code>1</code>, <code>r2</code> <code>=</code> <code>0</code>?—is <i>no</i>.

<p>
A good mental model for sequential consistency is to imagine all the processors
connected directly to the same shared memory, which can serve a read or write request
from one thread at a time.
There are no caches involved, so every time a processor needs to read from or write to
memory, that request goes to the shared memory.
The single-use-at-a-time shared memory imposes a sequential order on the
execution of all the memory accesses: sequential consistency.

<p>
<img name="mem-sc" class="center pad" width=482 height=95 src="mem-sc.png" srcset="mem-sc.png 1x, mem-sc@2x.png 2x, mem-sc@3x.png 3x, mem-sc@4x.png 4x">

<p>
(The three memory model hardware diagrams in this post are adapted from
Maranget <i>et</i> <i>al</i>., “<a href="https://www.cl.cam.ac.uk/~pes20/ppc-supplemental/test7.pdf">A Tutorial Introduction to the ARM and POWER Relaxed Memory Models</a>.”)

<p>
This diagram is a <i>model</i> for a sequentially consistent machine,
not the only way to build one.
Indeed, it is possible to build a sequentially consistent machine
using multiple shared memory modules
and caches to help predict the result of memory fetches,
but being sequentially consistent means
that machine must behave indistinguishably from this model.
If we are simply trying to understand what sequentially consistent
execution means,
we can ignore all of those possible implementation complications
and think about this one model.

<p>
Unfortunately for us as programmers,
giving up strict sequential consistency can let hardware execute programs faster,
so all modern hardware deviates in various ways from sequential consistency.
Defining exactly how specific hardware deviates
turns out to be quite difficult.
This post uses as two examples two memory models present
in today's widely-used hardware:
that of the x86,
and that of the ARM and POWER processor families.
<a class=anchor href="#x86"><h2 id="x86">x86 Total Store Order (x86-TSO)</h2></a>


<p>
The memory model for modern x86 systems corresponds to this
hardware diagram:

<p>
<img name="mem-tso" class="center pad" width=482 height=180 src="mem-tso.png" srcset="mem-tso.png 1x, mem-tso@2x.png 2x, mem-tso@3x.png 3x, mem-tso@4x.png 4x">

<p>
All the processors are still connected to a single shared memory,
but each processor queues writes to that memory in a local write queue.
The processor continues executing new instructions
while the writes make their way out to the shared memory.
A memory read on one processor consults the local write queue
before consulting main memory,
but it cannot see the write queues on other processors.
The effect is that a processor sees its own writes before others do.
But—and this is very important—all processors do agree on the
(total) order in which writes (stores) reach the shared memory,
giving the model its name: <i>total store order</i>, or TSO.
At the moment that a write reaches shared memory,
any future read on any processor will see it and use that value
(until it is overwritten by a later write, or perhaps by a buffered
write from another processor).

<p>
The write queue is a standard first-in, first-out queue:
the memory writes are applied to the shared memory in the same order
that they were executed by the processor.
Because the write order is preserved by the write queue,
and because other processors see the writes to shared memory
immediately, the message passing litmus test
we considered earlier has the same outcome as before:
<code>r1</code> <code>=</code> <code>1</code>, <code>r2</code> <code>=</code> <code>0</code> remains impossible.<blockquote>

<p>
<i>Litmus Test: Message Passing</i> <br>
Can this program see <code>r1</code> <code>=</code> <code>1</code>, <code>r2</code> <code>=</code> <code>0</code>?</blockquote>
<pre>// Thread 1           // Thread 2
x = 1                 r1 = y
y = 1                 r2 = x
</pre>
<blockquote>

<p>
On sequentially consistent hardware: no.<br>
On x86 (or other TSO): no.</blockquote>

<p>
The write queue guarantees that thread 1 writes <code>x</code> to memory before <code>y</code>,
and the system-wide agreement about the order of memory writes
(the total store order)
guarantees that thread 2 learns of <code>x</code>'s new value before it learns of <code>y</code>'s new value.
Therefore it is impossible for <code>r1</code> <code>=</code> <code>y</code> to see the new <code>y</code> without <code>r2</code> <code>=</code> <code>x</code>
also seeing the new <code>x</code>.
The store order is crucial here:
thread 1 writes <code>x</code> before <code>y</code>, so thread 2 must not see the write to <code>y</code> before the write to <code>x</code>.

<p>
The sequential consistency and TSO models agree in this case,
but they disagree about the results of other litmus tests.
For example, this is the usual example distinguishing the two models:<blockquote>

<p>
<i>Litmus Test: Write Queue (also called Store Buffer)</i> <br>
Can this program see <code>r1</code> <code>=</code> <code>0</code>, <code>r2</code> <code>=</code> <code>0</code>?</blockquote>
<pre>// Thread 1           // Thread 2
x = 1                 y = 1
r1 = y                r2 = x
</pre>
<blockquote>

<p>
On sequentially consistent hardware: no.<br>
On x86 (or other TSO): <i>yes!</i></blockquote>

<p>
In any sequentially consistent execution, either <code>x</code> <code>=</code> <code>1</code> or <code>y</code> <code>=</code> <code>1</code> must happen first,
and then the read in the other thread must observe it, so <code>r1</code> <code>=</code> <code>0</code>, <code>r2</code> <code>=</code> <code>0</code> is impossible.
But on a TSO system, it can happen that Thread 1 and Thread 2 both queue their writes
and then read from memory before either write makes it to memory,
so that both reads see zeros.

<p>
This example may seem artificial, but using two synchronization variables does happen in well-known synchronization algorithms, such as
<a href="https://en.wikipedia.org/wiki/Dekker%27s_algorithm">Dekker's algorithm</a>
or
<a href="https://en.wikipedia.org/wiki/Peterson%27s_algorithm">Peterson's algorithm</a>,
as well as ad hoc schemes.
They break if one thread isn’t seeing all the writes from another.

<p>
To fix algorithms that depend on stronger memory ordering,
non-sequentially-consistent hardware
supplies explicit instructions called memory barriers (or fences)
that can be used to control the ordering.
We can add a memory barrier to make sure that each thread
flushes its previous write to memory before starting its read:
<pre>// Thread 1           // Thread 2
x = 1                 y = 1
barrier               barrier
r1 = y                r2 = x
</pre>


<p>
With the addition of the right barriers, <code>r1</code> <code>=</code> <code>0</code>, <code>r2</code> <code>=</code> <code>0</code> is again impossible,
and Dekker's or Peterson's algorithm would then work correctly.
There are many kinds of barriers; the details vary from system to system
and are beyond the scope of this post.
The point is only that barriers exist and give programmers or language implementers
a way to force sequentially consistent behavior at critical moments
in a program.

<p>
One final example, to drive home why the model is called total store order.
In the model, there are local write queues but no caches on the read path.
Once a write reaches main memory, all processors not only agree that
the value is there but also agree about when it arrived relative to writes
from other processors.
Consider this litmus test:<blockquote>

<p>
<i>Litmus Test: Independent Reads of Independent Writes (IRIW)</i><br>
Can this program see <code>r1</code> <code>=</code> <code>1</code>, <code>r2</code> <code>=</code> <code>0</code>, <code>r3</code> <code>=</code> <code>1</code>, <code>r4</code> <code>=</code> <code>0</code>?<br>
(Can Threads 3 and 4 see <code>x</code> and <code>y</code> change in different orders?)</blockquote>
<pre>// Thread 1    // Thread 2    // Thread 3    // Thread 4
x = 1          y = 1          r1 = x         r3 = y
                              r2 = y         r4 = x
</pre>
<blockquote>

<p>
On sequentially consistent hardware: no.<br>
On x86 (or other TSO): no.</blockquote>

<p>
If Thread 3 sees <code>x</code> change before <code>y</code>, can Thread 4 see <code>y</code> change before <code>x</code>?
For x86 and other TSO machines,
the answer is no: there is a <i>total order</i> over all stores (writes)
to main memory, and all processors agree on that order,
subject to the wrinkle that each processor knows about its own writes before they reach main memory.
<a class=anchor href="#path_to_x86-tso"><h2 id="path_to_x86-tso">The Path to x86-TSO</h2></a>


<p>
The x86-TSO model seems fairly clean, but the path there was full of roadblocks and wrong turns.
In the 1990s, the manuals available for the first x86 multiprocessors said
next to nothing about the memory model provided by the hardware.

<p>
As one example of the problems,
Plan 9 was one of the first true multiprocessor operating systems
(without a global kernel lock) to run on the x86.
During the port to the multiprocessor Pentium Pro, in 1997,
the developers stumbled over unexpected behavior
that boiled down to the write queue litmus test.
A subtle piece of synchronization code assumed that
<code>r1</code> <code>=</code> <code>0</code>, <code>r2</code> <code>=</code> <code>0</code> was impossible, and yet it was happening.
Worse, the Intel manuals were vague about the memory model details.

<p>
In response to a mailing list suggestion that “it's better to be conservative
with locks than to trust hardware designers to do what we expect,”
one of the Plan 9 developers <a href="https://web.archive.org/web/20091124045026/http://9fans.net/archive/1997/04/76">explained the problem well</a>:<blockquote>

<p>
I certainly agree.  We are going to encounter more relaxed ordering
in multiprocessors.  The question is, what do the hardware
designers consider conservative?  Forcing an interlock
at both the beginning and end of a locked section seems to be
pretty conservative to me, but I clearly am not imaginative
enough.  The Pro manuals go into excruciating detail in describing
the caches and what keeps them coherent but don't seem to care
to say anything detailed about execution or read ordering.  The
truth is that we have no way of knowing whether we're conservative
enough.</blockquote>

<p>
During the discussion, an architect at Intel gave an informal
explanation of the memory model, pointing out that in theory
even multiprocessor 486 and Pentium systems could have produced
the <code>r1</code> <code>=</code> <code>0</code>, <code>r2</code> <code>=</code> <code>0</code> result, and that the Pentium Pro
simply had larger pipelines and write queues that exposed the
behavior more often.

<p>
The Intel architect also wrote:<blockquote>

<p>
Loosely speaking, this means the ordering of events originating
from any one processor in the system, as observed by other processors,
is always the same.  However, different observers are allowed
to disagree on the interleaving of events from two or more
processors.

<p>
Future Intel processors will implement the same memory ordering model.</blockquote>

<p>
The claim that “different observers are allowed
to disagree on the interleaving of events from two or more
processors” is saying that the answer to the IRIW litmus test
can answer “yes” on x86, even though in the previous section
we saw that x86 answers “no.”
How can that be?

<p>
The answer appears to be that Intel processors never actually
answered “yes” to that litmus test, but at the time the
Intel architects were reluctant to make any guarantee for future processors.
What little text existed in the architecture manuals made
almost no guarantees at all, making it very difficult to program against.

<p>
The Plan 9 discussion was not an isolated event.
The Linux kernel developers spent over a hundred messages
on their mailing list <a href="https://lkml.org/lkml/1999/11/20/76">starting in late November 1999</a> in similar confusion
over the guarantees provided by Intel processors.

<p>
In response to more and more people running into these difficulties
over the decade that followed,
a group of architects at Intel took on the task of writing down
useful guarantees about processor behavior, for both current and future processors.
The first result was the
“<a href="http://www.cs.cmu.edu/~410-f10/doc/Intel_Reordering_318147.pdf">Intel 64 Architecture Memory Ordering White Paper</a>”,
published in August 2007,
which aimed to “provide software writers with a clear understanding of
the results that different sequences of memory access instructions may produce.”
AMD published a similar description later that year in the
<a href="https://courses.cs.washington.edu/courses/cse351/12wi/supp-docs/AMD%20Vol%201.pdf"><i>AMD64 Architecture Programmer's Manual revision 3.14</i></a>.
These descriptions were based on a model called
“total lock order + causal consistency” (TLO+CC),
intentionally weaker than TSO.
In public talks, the Intel architects said that TLO+CC was
“as strong as required but no stronger.”
In particular, the model reserved the right for
x86 processors to answer “yes” to the IRIW litmus test.
Unfortunately, the definition of the memory barrier was
<a href="http://web.archive.org/web/20080512021617/http://blogs.sun.com/dave/entry/java_memory_model_concerns_on">not strong enough</a>
to reestablish sequentially-consistent memory semantics,
even with a barrier after every instruction.
Even worse,
researchers observed actual Intel x86 hardware
violating the TLO+CC model.
For example:<blockquote>

<p>
<i>Litmus Test: n6 (Paul Loewenstein)</i><br>
Can this program end with <code>r1</code> <code>=</code> <code>1</code>, <code>r2</code> <code>=</code> <code>0</code>, <code>x</code> <code>=</code> <code>1</code>?</blockquote>
<pre>// Thread 1    // Thread 2
x = 1          y = 1
r1 = x         x = 2
r2 = y
</pre>
<blockquote>

<p>
On sequentially consistent hardware: no.<br>
On x86 TLO+CC model (2007): no.<br>
On actual x86 hardware: <i>yes!</i><br>
On x86 TSO model: <i>yes!</i>
(Example from x86-TSO paper.)</blockquote>

<p>
Revisions to the Intel and AMD specifications later in 2008
guaranteed a “no” to the IRIW case and
strengthened the memory barriers
but still permitted unexpected behaviors that seem like
they could not arise on any reasonable hardware.
For example:<blockquote>

<p>
<i>Litmus Test: n5</i> <br>
Can this program end with <code>r1</code> <code>=</code> <code>2</code>, <code>r2</code> <code>=</code> <code>1</code>?</blockquote>
<pre>// Thread 1    // Thread 2
x = 1          x = 2
r1 = x         r2 = x
</pre>
<blockquote>

<p>
On sequentially consistent hardware: no.<br>
On x86 specification (2008): <i>yes!</i><br>
On actual x86 hardware: no.<br>
On x86 TSO model: no.
(Example from x86-TSO paper.)</blockquote>

<p>
To address these problems, Owens <i>et</i> <i>al</i>. <a href="https://www.cl.cam.ac.uk/~pes20/weakmemory/x86tso-paper.tphols.pdf">proposed the x86-TSO model</a>,
based on the earlier <a href="https://research.swtch.com/sparcv8.pdf">SPARCv8 TSO model</a>.
At they time they claimed that
“To the best of our knowledge, x86-TSO is sound, is strong enough to program
above, and is broadly in line with the vendors’ intentions.”
A few months later Intel and AMD released new manuals
broadly adopting this model.

<p>
It appears that all Intel processors did implement x86-TSO from the start,
even though it took a decade for Intel to decide to commit to that.
In retrospect, it is clear that the Intel and AMD architects were struggling
with exactly how to write a memory model that
left room for future processor optimizations while still
making useful guarantees for compiler writers and assembly-language programmers.
“As strong as required but no stronger” is a difficult balancing act.
<a class=anchor href="#relaxed"><h2 id="relaxed">ARM/POWER Relaxed Memory Model</h2></a>


<p>
Now let's look at an even more relaxed memory model,
the one found on ARM and POWER processors.
At an implementation level, these two systems are different in many ways,
but the guaranteed memory consistency model turns out to be roughly similar,
and quite a bit weaker than x86-TSO or even x86-TLO+CC.

<p>
The conceptual model for ARM and POWER systems is that each processor
reads from and writes to its own complete copy of memory,
and each write propagates to the other processors independently,
with reordering allowed as the writes propagate.

<p>
<img name="mem-weak" class="center pad" width=308 height=294 src="mem-weak.png" srcset="mem-weak.png 1x, mem-weak@2x.png 2x, mem-weak@3x.png 3x, mem-weak@4x.png 4x">

<p>
Here, there is no total store order.
Not depicted, each processor is also allowed to postpone a read
until it needs the result: a read can be delayed until after a later write.
In this relaxed model, the answer to every litmus test we’ve seen so far is “yes, that really can happen.”

<p>
For the original message passing litmus test,
the reordering of writes by a single processor
means that Thread 1's writes may not be observed
by other threads in the same order:<blockquote>

<p>
<i>Litmus Test: Message Passing</i><br>
Can this program see <code>r1</code> <code>=</code> <code>1</code>, <code>r2</code> <code>=</code> <code>0</code>?</blockquote>
<pre>// Thread 1           // Thread 2
x = 1                 r1 = y
y = 1                 r2 = x
</pre>
<blockquote>

<p>
On sequentially consistent hardware: no.<br>
On x86 (or other TSO): no.<br>
On ARM/POWER: <i>yes!</i></blockquote>

<p>
In the ARM/POWER model, we can think of thread 1 and thread 2 each having
their own separate copy of memory, with writes propagating
between the memories in any order whatsoever.
If thread 1's memory sends the update of <code>y</code> to thread 2 before sending the update of <code>x</code>,
and if thread 2 executes between those two updates, it will indeed see the result
<code>r1</code> <code>=</code> <code>1</code>, <code>r2</code> <code>=</code> <code>0</code>.

<p>
This result shows that the ARM/POWER memory model is weaker than TSO:
it makes fewer requirements on the hardware.
The ARM/POWER model still admits the kinds of reorderings that TSO does:<blockquote>

<p>
<i>Litmus Test: Store Buffering</i><br>
Can this program see <code>r1</code> <code>=</code> <code>0</code>, <code>r2</code> <code>=</code> <code>0</code>?</blockquote>
<pre>// Thread 1           // Thread 2
x = 1                 y = 1
r1 = y                r2 = x
</pre>
<blockquote>

<p>
On sequentially consistent hardware: no.<br>
On x86 (or other TSO): <i>yes!</i><br>
On ARM/POWER: <i>yes!</i></blockquote>

<p>
On ARM/POWER, the writes to <code>x</code> and <code>y</code> might be made to the
local memories but not yet have propagated when the
reads occur on the opposite threads.

<p>
Here’s the litmus test that showed what it meant for
x86 to have a total store order:<blockquote>

<p>
<i>Litmus Test: Independent Reads of Independent Writes (IRIW)</i> <br>
Can this program see <code>r1</code> <code>=</code> <code>1</code>, <code>r2</code> <code>=</code> <code>0</code>, <code>r3</code> <code>=</code> <code>1</code>, <code>r4</code> <code>=</code> <code>0</code>?<br>
(Can Threads 3 and 4 see <code>x</code> and <code>y</code> change in different orders?)</blockquote>
<pre>// Thread 1    // Thread 2    // Thread 3    // Thread 4
x = 1          y = 1          r1 = x         r3 = y
                              r2 = y         r4 = x
</pre>
<blockquote>

<p>
On sequentially consistent hardware: no.<br>
On x86 (or other TSO): no.<br>
On ARM/POWER: <i>yes!</i></blockquote>

<p>
On ARM/POWER, different threads may learn about different writes
in different orders.
They are not guaranteed to agree
about a total
order of writes reaching main memory, so Thread 3 can see <code>x</code> change before <code>y</code>
while Thread 4 sees <code>y</code> change before <code>x</code>.

<p>
As another example, ARM/POWER systems have visible buffering or reordering of memory reads (loads),
as demonstrated by this litmus test:<blockquote>

<p>
<i>Litmus Test: Load Buffering</i> <br>
Can this program see <code>r1</code> <code>=</code> <code>1</code>, <code>r2</code> <code>=</code> <code>1</code>? <br>
(Can each thread's read happen <i>after</i> the other thread's write?)</blockquote>
<pre>// Thread 1    // Thread 2
r1 = x         r2 = y
y = 1          x = 1
</pre>
<blockquote>

<p>
On sequentially consistent hardware: no.<br>
On x86 (or other TSO): no.<br>
On ARM/POWER: <i>yes!</i></blockquote>

<p>
Any sequentially consistent interleaving must start with
either thread 1's <code>r1</code> <code>=</code> <code>x</code> or thread 2's <code>r2</code> <code>=</code> <code>y</code>.
That read must see a zero, making the outcome <code>r1</code> <code>=</code> <code>1</code>, <code>r2</code> <code>=</code> <code>1</code> impossible.
In the ARM/POWER memory model, however, processors are
allowed to delay reads until after writes later in the instruction stream,
so that <code>y</code> <code>=</code> <code>1</code> and <code>x</code> <code>=</code> <code>1</code> execute <i>before</i> the two reads.

<p>
Although both the ARM and POWER memory models allow this result,
Maranget <i>et</i> <i>al</i>. <a href="https://www.cl.cam.ac.uk/~pes20/ppc-supplemental/test7.pdf">reported (in 2012)</a>
being able to reproduce it empirically
only on ARM systems, never on POWER.
Here the divergence between model and reality comes into play just as it did when we examined Intel x86:
hardware implementing a stronger model than technically guaranteed
encourages dependence on the stronger behavior
and means that future, weaker hardware will break programs,
validly or not.

<p>
Like on TSO systems, ARM and POWER have barriers that we can
insert into the examples above to force sequentially consistent
behaviors.
But the obvious question is whether ARM/POWER without barriers excludes any behavior at all.
Can the answer to any litmus test ever be “no, that can’t happen?”
It can, when we focus on a single memory location.

<p>
Here’s a litmus test for something that can’t happen even on ARM and POWER:<blockquote>

<p>
<i>Litmus Test: Coherence</i><br>
Can this program see <code>r1</code> <code>=</code> <code>1</code>, <code>r2</code> <code>=</code> <code>2</code>, <code>r3</code> <code>=</code> <code>2</code>, <code>r4</code> <code>=</code> <code>1</code>?<br>
(Can Thread 3 see <code>x</code> <code>=</code> <code>1</code> before <code>x</code> <code>=</code> <code>2</code> while Thread 4 sees the reverse?)</blockquote>
<pre>// Thread 1    // Thread 2    // Thread 3    // Thread 4
x = 1          x = 2          r1 = x         r3 = x
                              r2 = x         r4 = x
</pre>
<blockquote>

<p>
On sequentially consistent hardware: no.<br>
On x86 (or other TSO): no.<br>
On ARM/POWER: no.</blockquote>

<p>
This litmus test is like the previous one, but now
both threads are writing to a single variable <code>x</code>
instead of two distinct variables <code>x</code> and <code>y</code>.
Threads 1 and 2 write conflicting values 1 and 2 to <code>x</code>,
while Thread 3 and Thread 4 both read <code>x</code> twice.
If Thread 3 sees <code>x</code> <code>=</code> <code>1</code> overwritten by <code>x</code> <code>=</code> <code>2</code>,
can Thread 4 see the opposite?

<p>
The answer is no, even on ARM/POWER:
threads in the system must agree about a total order
for the writes to a single memory location.
That is, threads must agree which writes overwrite other writes.
This property is called called <i>coherence</i>.
Without the coherence property,
processors either disagree about the final result of memory
or else report a memory location flip-flopping from one value
to another and back to the first.
It would be very difficult to program such a system.

<p>
I'm purposely leaving out a lot of subtleties in the ARM and POWER weak memory models.
For more detail, see any of <a href="https://www.cl.cam.ac.uk/~pes20/papers/topics.html#Power_and_ARM">Peter Sewell's papers on the topic</a>.
Also, ARMv8 <a href="https://www.cl.cam.ac.uk/~pes20/armv8-mca/armv8-mca-draft.pdf">strengthened the memory model</a> by
making it “multicopy atomic,”
but I won't take the space here to explain exactly what that means.

<p>
There are two important points to take away.
First, there is an incredible amount of subtlety here,
the subject of well over a decade of academic research
by very persistent, very smart people.
I don't claim to understand anywhere near all of it myself.
This is not something we should hope to explain to ordinary programmers,
not something that we can hope to keep straight while debugging ordinary programs.
Second, the gap between what is allowed and what is observed
makes for unfortunate future surprises.
If current hardware does not exhibit the full range of allowed behaviors—especially
when it is difficult to reason about what is allowed in the first place!—then inevitably
programs will be written that accidentally depend on the more restricted behaviors
of the actual hardware.
If a new chip is less restricted in its behaviors, the fact that the new behavior
breaking your program is technically allowed by the hardware
memory model—that is, the bug is technically your fault—is of little consolation.
This is no way to write programs.
<a class=anchor href="#drf"><h2 id="drf">Weak Ordering and Data-Race-Free Sequential Consistency</h2></a>


<p>
By now I hope you're convinced that the hardware details
are complex and subtle and not something you want to work through
every time you write a program.
Instead, it would help to identify shortcuts of the form
“if you follow these easy rules, your program will only produce
results as if by some sequentially consistent interleaving.”
(We're still talking about hardware, so we're still talking
about interleaving individual assembly instructions.)

<p>
Sarita Adve and Mark Hill proposed exactly this approach
in their 1990 paper “<a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.42.5567">Weak Ordering – A New Definition</a>”. They defined “weakly ordered” as follows.<blockquote>

<p>
Let a synchronization model be a set of constraints
on memory accesses that specify how and when synchronization needs to be done.

<p>
Hardware is weakly ordered with respect to a synchronization model
if and only if it appears sequentially consistent to all software
that obey the synchronization model.</blockquote>

<p>
Although their paper was about capturing the hardware designs of that time
(not x86, ARM, and POWER),
the idea of elevating the discussion above specific designs,
keeps the paper relevant today.

<p>
I said before that
“valid optimizations do not change the behavior of valid programs.”
The rules define what valid means,
and then any hardware optimizations have to
keep those programs working as they might on
a sequentially consistent machine.
Of course, the interesting details are the rules themselves,
the constraints that define what it means for a program to be valid.

<p>
Adve and Hill propose one synchronization model, which they call <i>data-race-free (DRF)</i>.
This model assumes that hardware has memory synchronization operations
separate from ordinary memory reads and writes.
Ordinary memory reads and writes may be reordered between
synchronization operations, but they may not be moved across them.
(That is, the synchronization operations also serve as barriers to reordering.)
A program is said to be data-race-free
if, for all idealized sequentially consistent executions,
any two ordinary memory accesses to the same location
from different threads are either both reads or else
separated by synchronization operations forcing
one to happen before the other.

<p>
Let’s look at some examples, taken from Adve and Hill's paper
(redrawn for presentation).
Here is a single thread that executes a write of variable <code>x</code> followed by a read of the same variable.

<p>
<img name="mem-adve-1" class="center pad" width=80 height=101 src="mem-adve-1.png" srcset="mem-adve-1.png 1x, mem-adve-1@2x.png 2x, mem-adve-1@3x.png 3x, mem-adve-1@4x.png 4x">

<p>
The vertical arrow marks the order of execution within a single thread:
the write happens, then the read.
There is no race in this program, since everything is in a single thread.

<p>
In contrast, there is a race in this two-thread program:

<p>
<img name="mem-adve-2" class="center pad" width=208 height=101 src="mem-adve-2.png" srcset="mem-adve-2.png 1x, mem-adve-2@2x.png 2x, mem-adve-2@3x.png 3x, mem-adve-2@4x.png 4x">

<p>
Here, thread 2 writes to x without coordinating with thread 1.
Thread 2's write <i>races</i> with both the write and the read by thread 1.
If thread 2 were reading x instead of writing it,
the program would have only one race, between the write in thread 1
and the read in thread 2.
Every race involves at least one write: two uncoordinated reads do not race with each other.

<p>
To avoid races, we must add synchronization operations, which force
an order between operations on different threads
sharing a synchronization variable.
If the synchronization S(a) (synchronizing on variable a, marked by the dashed arrow) forces thread 2's write to happen after thread 1 is done,
the race is eliminated:

<p>
<img name="mem-adve-3" class="center pad" width=208 height=192 src="mem-adve-3.png" srcset="mem-adve-3.png 1x, mem-adve-3@2x.png 2x, mem-adve-3@3x.png 3x, mem-adve-3@4x.png 4x">

<p>
Now the write by thread 2 cannot happen at the same time as thread 1's operations.

<p>
If thread 2 were only reading, we would only need to synchronize with thread 1's write.
The two reads can still proceed concurrently:

<p>
<img name="mem-adve-4" class="center pad" width=208 height=147 src="mem-adve-4.png" srcset="mem-adve-4.png 1x, mem-adve-4@2x.png 2x, mem-adve-4@3x.png 3x, mem-adve-4@4x.png 4x">

<p>
Threads can be ordered by a sequence of synchronizations, even using an intermediate thread.
This program has no race:

<p>
<img name="mem-adve-5" class="center pad" width=336 height=192 src="mem-adve-5.png" srcset="mem-adve-5.png 1x, mem-adve-5@2x.png 2x, mem-adve-5@3x.png 3x, mem-adve-5@4x.png 4x">

<p>
On the other hand, the use of synchronization variables does not by itself
eliminate races: it is possible to use them incorrectly.
This program does have a race:

<p>
<img name="mem-adve-6" class="center pad" width=336 height=192 src="mem-adve-6.png" srcset="mem-adve-6.png 1x, mem-adve-6@2x.png 2x, mem-adve-6@3x.png 3x, mem-adve-6@4x.png 4x">

<p>
Thread 2's read is properly synchronized with the writes in the other threads—it definitely happens after both—but the
two writes are not themselves synchronized.
This program is <i>not</i> data-race-free.

<p>
Adve and Hill presented weak ordering as “a contract between software and hardware,”
specifically that if software avoids data races, then hardware acts as if it is
sequentially consistent, which is easier to reason about than the models we were examining
in the earlier sections.
But how can hardware satisfy its end of the contract?

<p>
Adve and Hill gave a proof that hardware “is weakly ordered by DRF,”
meaning it executes data-race-free programs as if by a sequentially consistent ordering,
provided it meets a set of certain minimum requirements.
I’m not going to go through the details, but the point
is that after the Adve and Hill paper,
hardware designers had a cookbook recipe backed by a proof:
do these things, and you can assert that
your hardware will appear sequentially consistent
to data-race-free programs.
And in fact, most relaxed hardware did behave this way and has continued to do so,
assuming appropriate implementations of the synchronization operations.
Adve and Hill were concerned originally with the VAX, but certainly x86, ARM, and POWER can satisfy these constraints too.
This idea that a system guarantees to data-race-free programs
the appearance of sequential consistency is often abbreviated <i>DRF-SC</i>.

<p>
DRF-SC marked a turning point in hardware memory models,
providing a clear strategy for both hardware designers and
software authors, at least those writing software in assembly language.
As we will see in the next post,
the question of a memory model for a higher-level programming language
does not have as neat and tidy an answer.

<p>
The next post in this series is about <a href="plmm">programming language memory models</a>.
<a class=anchor href="#acknowledgements"><h2 id="acknowledgements">Acknowledgements</h2></a>


<p>
This series of posts benefited greatly from discussions with
and feedback from a long list of engineers I am lucky to work with at Google.
My thanks to them.
I take full responsibility for any mistakes or unpopular opinions.
      </div>
      
    </div>

    
    
    
  </body>
</html>
















